{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad50fa47",
   "metadata": {},
   "source": [
    "# Theme 4: Convolutional Neural Networks (CNN)\n",
    "\n",
    "In this exercise set we use as a basis the same dataset and code as in Theme 3, but this time we extend it to a Convolutional Neural Network (CNN) for classification.\n",
    "\n",
    "## Goals\n",
    "1.  **CNN Basics**: Build a simple CNN with Strided Convolutions.\n",
    "2.  **Pooling**: Understand the effect of MaxPooling.\n",
    "3.  **Visualization**: Visualize learned kernels.\n",
    "4.  **Batch Normalization**: Improve training stability.\n",
    "5.  **Regularization**: Apply L2 regularization to CNNs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82228ea5",
   "metadata": {},
   "source": [
    "## 1. Imports\n",
    "\n",
    "We add `Conv2D`, `MaxPooling2D`, and `BatchNormalization` to our toolkit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf279c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.20.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Input, Dense, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155bedaa",
   "metadata": {},
   "source": [
    "## 2. Helper Functions\n",
    "\n",
    "In Deep Learning, data plumbing is often 80% of the work. We need robust functions to:\n",
    "- **Load Labels**: Read class IDs from text files.\n",
    "- **Load Images**: Read image files into NumPy arrays.\n",
    "- **Normalize**: Scale pixel values to a small range (e.g., -1 to 1 or 0 to 1) to help the optimizer converge.\n",
    "- **Split**: Separate data into Testing and Validation sets to ensure we don't cheat by optimizing on our test data.\n",
    "\n",
    "*Self-Learning Note*: Notice `normalize_dataset`. It shifts data to be 0-centered (approx) by subtracting 128 and dividing by 128. This helps gradient descent steps be more uniform.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "531dd840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Functions ---\n",
    "def load_labels(filename):\n",
    "    with open(filename,'r') as file:\n",
    "       li = file.readlines()\n",
    "    label_count = len(li)\n",
    "    labels = np.empty((label_count,1), dtype='int')\n",
    "    i = 0\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            labels[i] = int(line.replace(\"\\n\", \"\"))\n",
    "            i = i + 1\n",
    "    return labels\n",
    "\n",
    "def load_images(folder, image_count, image_size):\n",
    "    array_shape = (image_count, image_size[0], image_size[1], image_size[2])\n",
    "    imageset = np.empty(array_shape, dtype='float')\n",
    "    for i in range(0,image_count):\n",
    "        fname = os.path.join(folder, 'image_' + \"{:04d}\".format(i) + '.png')\n",
    "        if not os.path.exists(fname):\n",
    "             print(f\"Warning: File {fname} not found.\")\n",
    "             continue\n",
    "        image = Image.open(fname)\n",
    "        imageset[i] = np.asarray(image)\n",
    "    return imageset\n",
    "\n",
    "def normalize_dataset(sampled_images):\n",
    "    # Scale to range [-1, 1] approx\n",
    "    sampled_images = (sampled_images.astype('float32')-128) / 128\n",
    "    return sampled_images\n",
    "\n",
    "def split_test_val(data, splitpoint):\n",
    "    # Returns (Tail, Head) - careful with indices!\n",
    "    return data[splitpoint:], data[:splitpoint]\n",
    "\n",
    "def create_model(input_shape, dense_size, classes, l2_reg=None):\n",
    "    x = Input(shape=(input_shape))\n",
    "    y = Flatten()(x)\n",
    "    \n",
    "    if l2_reg:\n",
    "        y = Dense(classes, activation='softmax', name='dense_layer', kernel_regularizer=l2_reg)(y)\n",
    "    else:\n",
    "        y = Dense(classes, activation='softmax', name='dense_layer')(y)\n",
    "        \n",
    "    model = Model(inputs=x, outputs=y)\n",
    "    return model\n",
    "\n",
    "def create_model_2_layers(input_shape, dense_size, classes):\n",
    "    x = Input(shape=(input_shape))\n",
    "    y = Flatten()(x)\n",
    "    # Task 7: 2nd dense layer usually adds 'depth' allowing complex features\n",
    "    y = Dense(dense_size, activation='relu', name='dense_layer_1')(y)\n",
    "    y = Dense(classes, activation='softmax', name='dense_layer_out')(y)\n",
    "    model = Model(inputs=x, outputs=y)\n",
    "    return model\n",
    "\n",
    "def visualize_weights(model, layer_name, title, filename):\n",
    "    try:\n",
    "        weights, biases = model.get_layer(layer_name).get_weights()\n",
    "    except ValueError:\n",
    "        print(f\"Layer {layer_name} not found or has no weights\")\n",
    "        return\n",
    "\n",
    "    # Weights shape: (InputDim, Units) -> (3072, 10)\n",
    "    input_shape = (32, 32, 3)\n",
    "    n_units = weights.shape[1]\n",
    "    \n",
    "    # We want to show the weights for each class (unit)\n",
    "    rows = 2\n",
    "    cols = 5 \n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(10, 5))\n",
    "    \n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i >= n_units:\n",
    "            break\n",
    "\n",
    "        weight_vector = weights[:, i]\n",
    "        # Reshape: (3072,) -> (32, 32, 3)\n",
    "        weight_image = weight_vector.reshape(input_shape)\n",
    "\n",
    "        # Normalize to 0-1 for display\n",
    "        min_val = weight_image.min()\n",
    "        max_val = weight_image.max()\n",
    "        weight_image = (weight_image - min_val) / (max_val - min_val + 1e-5)\n",
    "\n",
    "        ax.imshow(weight_image)\n",
    "        ax.set_title(f'Class {i}')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show() # Changed from savefig to show for notebook\n",
    "    print(f\"Visualization generated.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2494807",
   "metadata": {},
   "source": [
    "## 3. Loading Data\n",
    "\n",
    "We load the CIFAR-10 subset.\n",
    "We assume the `resources` folder is in the current working directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "388417d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Data ---\n",
      "Data Loaded.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Loading Data ---\")\n",
    "base_res = 'resources' # Assumed relative to CWD\n",
    "if not os.path.exists(base_res):\n",
    "    print(f\"ERROR: '{base_res}' directory not found. Please ensure you are in the correct directory.\")\n",
    "else:\n",
    "    # 1. Load Labels\n",
    "    y_train = load_labels(os.path.join(base_res, 'training/labels.csv'))\n",
    "    \n",
    "    # 2. Load Images (based on how many labels we have)\n",
    "    x_train_raw = load_images(os.path.join(base_res, 'training'), len(y_train), (32,32,3))\n",
    "    \n",
    "    Y_test = load_labels(os.path.join(base_res, 'testing/labels.csv'))\n",
    "    X_test_raw = load_images(os.path.join(base_res, 'testing'), len(Y_test), (32,32,3))\n",
    "\n",
    "    print(\"Data Loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004bca26",
   "metadata": {},
   "source": [
    "## 4. Normalization & Splitting\n",
    "\n",
    "**Normalization**: Raw pixel values are 0-255. Neural networks prefer inputs near 0 with unit variance. We subtract 128 and divide by 128.\n",
    "\n",
    "**Splitting**: We split the 'Test' set folder into a true 'Test' set and a 'Validation' set.\n",
    "- **Validation**: Used during training to check progress.\n",
    "- **Test**: Used ONLY at the very end to report final performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4be72ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (10000, 32, 32, 3)\n",
      "Validation set: (2000, 32, 32, 3)\n",
      "Test set: (3000, 32, 32, 3)\n",
      "Classes: 10\n"
     ]
    }
   ],
   "source": [
    "x_train = normalize_dataset(x_train_raw)\n",
    "X_test = normalize_dataset(X_test_raw)\n",
    "\n",
    "# Splitting logic from original script\n",
    "# index 0 to 2000 -> Validation\n",
    "# index 2000 to End -> Test\n",
    "splitpoint = 2000\n",
    "x_test, x_val = split_test_val(X_test, splitpoint)\n",
    "y_test, y_val = split_test_val(Y_test, splitpoint)\n",
    "\n",
    "class_count = len(np.unique(y_train))\n",
    "dims = (32, 32, 3)\n",
    "dense_sz = 100\n",
    "\n",
    "print(f\"Training set: {x_train.shape}\")\n",
    "print(f\"Validation set: {x_val.shape}\")\n",
    "print(f\"Test set: {x_test.shape}\")\n",
    "print(f\"Classes: {class_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a980c476",
   "metadata": {},
   "source": [
    "## 2. First CNN: Strided Convolutions\n",
    "\n",
    "\n",
    "\n",
    "Instead of flattening the image immediately, we use `Conv2D` layers to extract spatial features.\n",
    "\n",
    "We use `strides=2` to downsample the efficient spatial dimensions instead of pooling.\n",
    "\n",
    "\n",
    "\n",
    "**Architecture**:\n",
    "\n",
    "1.  Input (32, 32, 3)\n",
    "\n",
    "2.  Conv2D (32 filters, 3x3, stride=2, padding='same')\n",
    "\n",
    "3.  Conv2D (64 filters, 3x3, stride=2, padding='same')\n",
    "\n",
    "4.  Flatten\n",
    "\n",
    "5.  Dense (100)\n",
    "\n",
    "6.  Dense (10 classses)\n",
    "\n",
    "\n",
    "\n",
    "**Hyperparameters**:\n",
    "\n",
    "-   Epochs: 12\n",
    "\n",
    "-   Learning Rate: 3e-4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8c3c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_strided(input_shape, dense_size, classes):\n",
    "\n",
    "    kernel_sz = (3, 3)\n",
    "\n",
    "    x = Input(shape=input_shape)\n",
    "\n",
    "    # 1st Conv Block\n",
    "\n",
    "    y = Conv2D(filters=32, kernel_size=kernel_sz, strides=2, activation='relu', \n",
    "\n",
    "               padding='same', kernel_initializer='he_normal')(x)\n",
    "\n",
    "    # 2nd Conv Block\n",
    "\n",
    "    y = Conv2D(filters=64, kernel_size=kernel_sz, strides=2, activation='relu', \n",
    "\n",
    "               padding='same', kernel_initializer='he_normal')(y)\n",
    "\n",
    "    \n",
    "\n",
    "    y = Flatten()(y)\n",
    "\n",
    "    y = Dense(dense_size, activation='relu')(y)\n",
    "\n",
    "    y = Dense(classes, activation='softmax')(y)\n",
    "\n",
    "    \n",
    "\n",
    "    model = Model(inputs=x, outputs=y)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "print(\"--- Training Strided CNN ---\")\n",
    "\n",
    "model_stride = create_cnn_strided(dims, dense_sz, class_count)\n",
    "\n",
    "model_stride.compile(loss='sparse_categorical_crossentropy', \n",
    "\n",
    "                     optimizer=tf.keras.optimizers.Adam(learning_rate=3e-4), \n",
    "\n",
    "                     metrics=['accuracy'])\n",
    "\n",
    "model_stride.summary()\n",
    "\n",
    "\n",
    "\n",
    "history_stride = model_stride.fit(x_train, y_train, epochs=12, validation_data=(x_val, y_val), verbose=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dea4592",
   "metadata": {},
   "source": [
    "## 3. Evaluation\n",
    "\n",
    "\n",
    "\n",
    "Compare the test accuracy with the Linear Network from Theme 3 (approx. 35-40%).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2873c626",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model_stride.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print(f'Test accuracy (Strided CNN): {score[1]:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49e446b",
   "metadata": {},
   "source": [
    "## 4. CNN with MaxPooling\n",
    "\n",
    "\n",
    "\n",
    "Now we switch to `strides=1` (no downsampling in convolution) and add explicit `MaxPooling2D` layers to reduce dimensions.\n",
    "\n",
    "\n",
    "\n",
    "**Architecture Change**:\n",
    "\n",
    "-   Conv2D (stride=1) -> MaxPooling2D\n",
    "\n",
    "-   Conv2D (stride=1) -> MaxPooling2D\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dbec74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_pooling(input_shape, dense_size, classes):\n",
    "\n",
    "    kernel_sz = (3, 3)\n",
    "\n",
    "    x = Input(shape=input_shape)\n",
    "\n",
    "    \n",
    "\n",
    "    # Block 1\n",
    "\n",
    "    y = Conv2D(filters=32, kernel_size=kernel_sz, strides=1, activation='relu', \n",
    "\n",
    "               padding='same', kernel_initializer='he_normal')(x)\n",
    "\n",
    "    y = MaxPooling2D()(y)\n",
    "\n",
    "    \n",
    "\n",
    "    # Block 2\n",
    "\n",
    "    y = Conv2D(filters=64, kernel_size=kernel_sz, strides=1, activation='relu', \n",
    "\n",
    "               padding='same', kernel_initializer='he_normal')(y)\n",
    "\n",
    "    y = MaxPooling2D()(y)\n",
    "\n",
    "    \n",
    "\n",
    "    y = Flatten()(y)\n",
    "\n",
    "    y = Dense(dense_size, activation='relu')(y)\n",
    "\n",
    "    y = Dense(classes, activation='softmax')(y)\n",
    "\n",
    "    \n",
    "\n",
    "    model = Model(inputs=x, outputs=y)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "print(\"--- Training CNN with MaxPooling ---\")\n",
    "\n",
    "model_pool = create_cnn_pooling(dims, dense_sz, class_count)\n",
    "\n",
    "model_pool.compile(loss='sparse_categorical_crossentropy', \n",
    "\n",
    "                   optimizer=tf.keras.optimizers.Adam(learning_rate=3e-4), \n",
    "\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "history_pool = model_pool.fit(x_train, y_train, epochs=12, validation_data=(x_val, y_val), verbose=2)\n",
    "\n",
    "score_pool = model_pool.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print(f'Test accuracy (Pooling CNN): {score_pool[1]:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3fb60a",
   "metadata": {},
   "source": [
    "## 5. Visualizing Kernels\n",
    "\n",
    "\n",
    "\n",
    "We visualize the 32 filters of the first layer.\n",
    "\n",
    "Since the input is RGB (3 channels), each filter is 3x3x3.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b83943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_conv_weights(model, layer_index=1):\n",
    "\n",
    "    # Get weights\n",
    "\n",
    "    # Note: Keras layer indexing might vary. Usually Input is 0.\n",
    "\n",
    "    # We can also get by name if we named them, but let's grab the first Conv2D.\n",
    "\n",
    "    conv_layers = [l for l in model.layers if isinstance(l, Conv2D)]\n",
    "\n",
    "    if not conv_layers:\n",
    "\n",
    "        print(\"No Conv2D layers found.\")\n",
    "\n",
    "        return\n",
    "\n",
    "    \n",
    "\n",
    "    target_layer = conv_layers[0]\n",
    "\n",
    "    weights = target_layer.get_weights()[0] # [0] is weights, [1] is biases\n",
    "\n",
    "    print(f\"First Conv2D Layer Weight Shape: {weights.shape}\") \n",
    "\n",
    "    # Shape is (KernelH, KernelW, InputChannels, Filters) e.g. (3, 3, 3, 32)\n",
    "\n",
    "    \n",
    "\n",
    "    n_filters = weights.shape[3]\n",
    "\n",
    "    \n",
    "\n",
    "    # Setup plot\n",
    "\n",
    "    cols = 8\n",
    "\n",
    "    rows = np.ceil(n_filters / cols).astype(int)\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(12, rows*1.5))\n",
    "\n",
    "    \n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "\n",
    "        if i >= n_filters:\n",
    "\n",
    "            break\n",
    "\n",
    "        \n",
    "\n",
    "        # Get the filter: (3, 3, 3)\n",
    "\n",
    "        f = weights[:, :, :, i]\n",
    "\n",
    "        \n",
    "\n",
    "        # Normalize to 0-1 for display\n",
    "\n",
    "        f_min, f_max = f.min(), f.max()\n",
    "\n",
    "        f_disp = (f - f_min) / (f_max - f_min + 1e-5)\n",
    "\n",
    "        \n",
    "\n",
    "        ax.imshow(f_disp, interpolation='nearest')\n",
    "\n",
    "        ax.axis('off')\n",
    "\n",
    "        ax.set_title(f'#{i}')\n",
    "\n",
    "        \n",
    "\n",
    "    plt.suptitle(f\"Kernels of {target_layer.name}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "visualize_conv_weights(model_pool)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c682036d",
   "metadata": {},
   "source": [
    "## 6. Memory Usage\n",
    "\n",
    "\n",
    "\n",
    "**Parameter Count**:\n",
    "\n",
    "-   Conv2D parameters = `(kernel_h * kernel_w * input_channels + 1) * filters`\n",
    "\n",
    "-   Dense parameters = `(input_units + 1) * output_units`\n",
    "\n",
    "\n",
    "\n",
    "**Activation Memory** (Forward Pass):\n",
    "\n",
    "-   Layer Output Bytes = `Height * Width * Filters * 4 bytes (float32)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75815aac",
   "metadata": {},
   "source": [
    "## 7. Batch Normalization\n",
    "\n",
    "\n",
    "\n",
    "We add `BatchNormalization()` after each `Conv2D` and `Dense` layer to normalize activation distributions.\n",
    "\n",
    "This often allows higher learning rates and faster convergence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb396941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_bn(input_shape, dense_size, classes):\n",
    "\n",
    "    kernel_sz = (3, 3)\n",
    "\n",
    "    x = Input(shape=input_shape)\n",
    "\n",
    "    \n",
    "\n",
    "    # Block 1\n",
    "\n",
    "    y = Conv2D(filters=32, kernel_size=kernel_sz, strides=1, activation='relu', \n",
    "\n",
    "               padding='same', kernel_initializer='he_normal')(x)\n",
    "\n",
    "    y = BatchNormalization()(y)\n",
    "\n",
    "    y = MaxPooling2D()(y)\n",
    "\n",
    "    \n",
    "\n",
    "    # Block 2\n",
    "\n",
    "    y = Conv2D(filters=64, kernel_size=kernel_sz, strides=1, activation='relu', \n",
    "\n",
    "               padding='same', kernel_initializer='he_normal')(y)\n",
    "\n",
    "    y = BatchNormalization()(y)\n",
    "\n",
    "    y = MaxPooling2D()(y)\n",
    "\n",
    "    \n",
    "\n",
    "    y = Flatten()(y)\n",
    "\n",
    "    y = Dense(dense_size, activation='relu')(y)\n",
    "\n",
    "    y = BatchNormalization()(y)\n",
    "\n",
    "    \n",
    "\n",
    "    y = Dense(classes, activation='softmax')(y)\n",
    "\n",
    "    \n",
    "\n",
    "    model = Model(inputs=x, outputs=y)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "print(\"--- Training CNN with BN ---\")\n",
    "\n",
    "model_bn = create_cnn_bn(dims, dense_sz, class_count)\n",
    "\n",
    "model_bn.compile(loss='sparse_categorical_crossentropy', \n",
    "\n",
    "                 optimizer=tf.keras.optimizers.Adam(learning_rate=3e-4), \n",
    "\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "history_bn = model_bn.fit(x_train, y_train, epochs=12, validation_data=(x_val, y_val), verbose=2)\n",
    "\n",
    "score_bn = model_bn.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print(f'Test accuracy (BN): {score_bn[1]:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe78af1",
   "metadata": {},
   "source": [
    "## 8. L2 Regularization\n",
    "\n",
    "\n",
    "\n",
    "We apply L2 regularization to weights to prevent overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d305131f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_l2(input_shape, dense_size, classes, l2_strength=1e-4):\n",
    "\n",
    "    kernel_sz = (3, 3)\n",
    "\n",
    "    reg = regularizers.l2(l2_strength)\n",
    "\n",
    "    x = Input(shape=input_shape)\n",
    "\n",
    "    \n",
    "\n",
    "    # Block 1\n",
    "\n",
    "    y = Conv2D(filters=32, kernel_size=kernel_sz, strides=1, activation='relu', \n",
    "\n",
    "               padding='same', kernel_initializer='he_normal', kernel_regularizer=reg)(x)\n",
    "\n",
    "    y = BatchNormalization()(y)\n",
    "\n",
    "    y = MaxPooling2D()(y)\n",
    "\n",
    "    \n",
    "\n",
    "    # Block 2\n",
    "\n",
    "    y = Conv2D(filters=64, kernel_size=kernel_sz, strides=1, activation='relu', \n",
    "\n",
    "               padding='same', kernel_initializer='he_normal', kernel_regularizer=reg)(y)\n",
    "\n",
    "    y = BatchNormalization()(y)\n",
    "\n",
    "    y = MaxPooling2D()(y)\n",
    "\n",
    "    \n",
    "\n",
    "    y = Flatten()(y)\n",
    "\n",
    "    y = Dense(dense_size, activation='relu', kernel_regularizer=reg)(y)\n",
    "\n",
    "    y = BatchNormalization()(y)\n",
    "\n",
    "    \n",
    "\n",
    "    y = Dense(classes, activation='softmax', kernel_regularizer=reg)(y)\n",
    "\n",
    "    \n",
    "\n",
    "    model = Model(inputs=x, outputs=y)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "print(\"--- Training CNN with L2 ---\")\n",
    "\n",
    "model_l2 = create_cnn_l2(dims, dense_sz, class_count, l2_strength=0.001)\n",
    "\n",
    "model_l2.compile(loss='sparse_categorical_crossentropy', \n",
    "\n",
    "                 optimizer=tf.keras.optimizers.Adam(learning_rate=3e-4), \n",
    "\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "history_l2 = model_l2.fit(x_train, y_train, epochs=12, validation_data=(x_val, y_val), verbose=2)\n",
    "\n",
    "score_l2 = model_l2.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print(f'Test accuracy (L2): {score_l2[1]:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
