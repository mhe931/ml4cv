{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95bbd6cc",
   "metadata": {},
   "source": [
    "# Theme 3: Inspecting Neural Network Functionality (Julia/Flux)\n",
    "\n",
    "This notebook implements the exercises for Theme 3 using the Julia programming language and the Flux.jl machine learning library. We will inspect the functionality of a neural network in image classification using the CIFAR-10 dataset.\n",
    "\n",
    "## Setup\n",
    "We need the following packages: `Flux`, `Images`, `FileIO`, `Statistics`, `Plots`, `LinearAlgebra`, `Random`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6adba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "Pkg.add([\"Flux\", \"Images\", \"FileIO\", \"Plots\", \"OneHotArrays\", \"FileIO\", \"ImageIO\", \"ImageMagick\"])\n",
    "# Note: ImageMagick/ImageIO might be needed for PNG loading depending on OS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e11702",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux\n",
    "using Flux: onehotbatch, onecold, crossentropy, train!, @epochs, params\n",
    "using Images\n",
    "using FileIO\n",
    "using Statistics\n",
    "using Plots\n",
    "using LinearAlgebra\n",
    "using Random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ad6500",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "\n",
    "We define helper functions to load the CIFAR-10 small dataset.\n",
    "**Note**: Flux typically expects image data in the format `(Width, Height, Channels, BatchSize)`. We ensure our data loader adheres to this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a4e34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "function load_labels_jl(filename)\n",
    "    if !isfile(filename)\n",
    "        println(\"Warning: Label file $filename not found.\")\n",
    "        return Int[]\n",
    "    end\n",
    "    lines = readlines(filename)\n",
    "    return parse.(Int, lines)\n",
    "end\n",
    "\n",
    "function load_image_batch(folder, count, img_size=(32,32))\n",
    "    # Initialize array: (Width, Height, Channels, BatchSize)\n",
    "    data = zeros(Float32, img_size[1], img_size[2], 3, count)\n",
    "    \n",
    "    for i in 0:(count-1)\n",
    "        fname = joinpath(folder, \"image_\" * lpad(i, 4, \"0\") * \".png\")\n",
    "        if !isfile(fname)\n",
    "            continue\n",
    "        end\n",
    "        img = load(fname)\n",
    "        # Permute from (H, W) standard to (W, H) if needed, or just ensure channel dim is last in intermediate \n",
    "        # Flux Images are typically (W, H, C, N). \n",
    "        # Image loading gives (C, H, W) often.\n",
    "        mat = channelview(img) \n",
    "        mat = permutedims(mat, (3, 2, 1)) # -> (W, H, C)\n",
    "        \n",
    "        # Scaling: (x - 128) / 128 (performed later in batch, or here raw 0-255)\n",
    "        # Load gives 0..1. We want 0..255 for the formula equivalent.\n",
    "        data[:, :, :, i+1] = float.(mat) .* 255\n",
    "    end\n",
    "    return data\n",
    "end\n",
    "\n",
    "function normalize_dataset(data)\n",
    "    return (data .- 128.0f0) ./ 128.0f0\n",
    "end\n",
    "\n",
    "function get_data()\n",
    "    base_res = \"resources\"\n",
    "    train_labels_path = joinpath(base_res, \"training\", \"labels.csv\")\n",
    "    train_img_path = joinpath(base_res, \"training\")\n",
    "    test_labels_path = joinpath(base_res, \"testing\", \"labels.csv\")\n",
    "    test_img_path = joinpath(base_res, \"testing\")\n",
    "\n",
    "    y_train_raw = load_labels_jl(train_labels_path)\n",
    "    # Only load what we have\n",
    "    if isempty(y_train_raw)\n",
    "        println(\"Error: No training labels found. Check path.\")\n",
    "        return nothing\n",
    "    end\n",
    "    \n",
    "    x_train_raw = load_image_batch(train_img_path, length(y_train_raw))\n",
    "    x_train = normalize_dataset(x_train_raw)\n",
    "    \n",
    "    y_test_full = load_labels_jl(test_labels_path)\n",
    "    x_test_full_raw = load_image_batch(test_img_path, length(y_test_full))\n",
    "    x_test_norm = normalize_dataset(x_test_full_raw)\n",
    "    \n",
    "    # Validation Split\n",
    "    splitpoint = 2000\n",
    "    x_val = x_test_norm[:, :, :, 1:splitpoint]\n",
    "    y_val_raw = y_test_full[1:splitpoint]\n",
    "    \n",
    "    x_test = x_test_norm[:, :, :, splitpoint+1:end]\n",
    "    y_test_raw = y_test_full[splitpoint+1:end]\n",
    "    \n",
    "    # One Hot Encoding\n",
    "    classes = sort(unique(y_train_raw))\n",
    "    y_train = onehotbatch(y_train_raw, classes)\n",
    "    y_val = onehotbatch(y_val_raw, classes)\n",
    "    y_test = onehotbatch(y_test_raw, classes)\n",
    "    \n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test, length(classes)\n",
    "end\n",
    "\n",
    "# Load the data\n",
    "print(\"Loading data... \")\n",
    "data = get_data()\n",
    "if data !== nothing\n",
    "    x_train, y_train, x_val, y_val, x_test, y_test, class_count = data\n",
    "    println(\"Done.\")\n",
    "    println(\"Train shape: \", size(x_train))\n",
    "    println(\"Val shape: \", size(x_val))\n",
    "    println(\"Test shape: \", size(x_test))\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3817f421",
   "metadata": {},
   "source": [
    "## 2. Task 3: Linear Model (5 Epochs)\n",
    "\n",
    "We define a simple linear model: `Input -> Flatten -> Dense -> Softmax`.\n",
    "**Question**: How many parameters?\n",
    "**Answer**: $3072 \times 10 + 10 = 30,730$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412675c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 32 * 32 * 3\n",
    "\n",
    "model_t3 = Chain(\n",
    "    Flux.flatten,\n",
    "    Dense(input_dim, class_count, softmax)\n",
    ")\n",
    "\n",
    "loss(x, y) = crossentropy(model_t3(x), y)\n",
    "opt = Adam(3e-5)\n",
    "data_loader = Flux.DataLoader((x_train, y_train), batchsize=32, shuffle=true)\n",
    "\n",
    "function evaluate_acc(m, x, y)\n",
    "    y_hat = m(x)\n",
    "    return mean(onecold(y_hat) .== onecold(y))\n",
    "end\n",
    "\n",
    "println(\"Training Task 3 (5 epochs)...\")\n",
    "for epoch in 1:5\n",
    "    Flux.train!(loss, params(model_t3), data_loader, opt)\n",
    "    val_acc = evaluate_acc(model_t3, x_val, y_val)\n",
    "    println(\"Epoch $epoch: Val Acc = $val_acc\")\n",
    "end\n",
    "\n",
    "test_acc_t3 = evaluate_acc(model_t3, x_test, y_test)\n",
    "println(\"Task 3 Test Accuracy: $test_acc_t3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff874110",
   "metadata": {},
   "source": [
    "## 3. Task 4: Extended Training (100 Epochs)\n",
    "\n",
    "We train the same model for 100 epochs to observe overfitting behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713dccbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t4 = Chain(\n",
    "    Flux.flatten,\n",
    "    Dense(input_dim, class_count, softmax)\n",
    ")\n",
    "loss4(x, y) = crossentropy(model_t4(x), y)\n",
    "opt4 = Adam(3e-5)\n",
    "\n",
    "history_train = Float64[]\n",
    "history_val = Float64[]\n",
    "\n",
    "println(\"Training Task 4 (100 epochs)...\")\n",
    "for epoch in 1:100\n",
    "    Flux.train!(loss4, params(model_t4), data_loader, opt4)\n",
    "    push!(history_train, evaluate_acc(model_t4, x_train, y_train))\n",
    "    push!(history_val, evaluate_acc(model_t4, x_val, y_val))\n",
    "    if epoch % 10 == 0\n",
    "        println(\"Epoch $epoch: Val Acc = $(history_val[end])\")\n",
    "    end\n",
    "end\n",
    "\n",
    "p4 = plot(history_train, label=\"Train Acc\", title=\"Task 4: Accuracy vs Epochs\", xlabel=\"Epoch\", ylabel=\"Accuracy\", linewidth=2)\n",
    "plot!(p4, history_val, label=\"Val Acc\", linewidth=2)\n",
    "display(p4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983107d7",
   "metadata": {},
   "source": [
    "**Observation**: The training accuracy should continue to rise while validation accuracy plateaus, indicating overfitting. The model is memorizing the noise in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45acbb6e",
   "metadata": {},
   "source": [
    "## 4. Task 5: Weight Visualization\n",
    "\n",
    "We visualize the learned weights of the Dense layer. The weights, when reshaping back to image dimensions, often look like \"templates\" of the classes they represent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7289086d",
   "metadata": {},
   "outputs": [],
   "source": [
    "function visualize_weights(model, layer_idx, title)\n",
    "    W = model[layer_idx].weight\n",
    "    n_classes = size(W, 1)\n",
    "    \n",
    "    plots = []\n",
    "    for i in 1:n_classes\n",
    "        w_vec = W[i, :]\n",
    "        w_img = reshape(w_vec, 32, 32, 3)\n",
    "        \n",
    "        # Normalize 0-1\n",
    "        w_min, w_max = extrema(w_img)\n",
    "        w_norm = (w_img .- w_min) ./ (w_max - w_min + 1e-5)\n",
    "        \n",
    "        # Permute to (H, W, C) for plotting (Plots.jl expects H,W for images sometimes, or just standard RGB array)\n",
    "        # Image structure in Julia is typically handled by `colorview(RGB, ...)` expecting (Channels, Height, Width) usually?\n",
    "        # Let's try (3, 32, 32)\n",
    "        img_c = permutedims(w_norm, (3, 2, 1))\n",
    "        p = plot(colorview(RGB, img_c), axis=false, title=\"Class $(i-1)\", ticks=false)\n",
    "        push!(plots, p)\n",
    "    end\n",
    "    display(plot(plots..., layout=(2, 5), size=(800, 300), title=title))\n",
    "end\n",
    "\n",
    "visualize_weights(model_t4, 2, \"Task 5: Weights\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263c63be",
   "metadata": {},
   "source": [
    "## 5. Task 6: L2 Regularization\n",
    "\n",
    "We add L2 regularization to the loss function to penalize large weights.\n",
    "`Loss = CrossEntropy + lambda * sum(weights^2)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa34e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_l2 = Chain(\n",
    "    Flux.flatten,\n",
    "    Dense(input_dim, class_count, softmax)\n",
    ")\n",
    "\n",
    "l2_lambda = 0.03\n",
    "function loss_l2(x, y)\n",
    "    weights = model_l2[2].weight\n",
    "    return crossentropy(model_l2(x), y) + l2_lambda * sum(abs2, weights)\n",
    "end\n",
    "\n",
    "opt_l2 = Adam(3e-5)\n",
    "\n",
    "println(\"Training Task 6 (L2 Regularization)...\")\n",
    "for epoch in 1:100\n",
    "    Flux.train!(loss_l2, params(model_l2), data_loader, opt_l2)\n",
    "end\n",
    "\n",
    "test_acc_l2 = evaluate_acc(model_l2, x_test, y_test)\n",
    "println(\"Task 6 Test Accuracy: $test_acc_l2\")\n",
    "\n",
    "visualize_weights(model_l2, 2, \"Task 6: L2 Weights\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e723ddb2",
   "metadata": {},
   "source": [
    "**Observation**: The weights should look smoother and less noisy compared to Task 5 (\"ghostly templates\"). The accuracy should generalize better than the unregularized model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cd5324",
   "metadata": {},
   "source": [
    "## 6. Task 7: 2-Layer Network\n",
    "\n",
    "We introduce a hidden layer with 100 neurons and ReLU activation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8027b7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_size = 100\n",
    "model_2l = Chain(\n",
    "    Flux.flatten,\n",
    "    Dense(input_dim, dense_size, relu),\n",
    "    Dense(dense_size, class_count, softmax)\n",
    ")\n",
    "\n",
    "loss_2l(x, y) = crossentropy(model_2l(x), y)\n",
    "opt_2l = Adam(3e-5)\n",
    "\n",
    "println(\"Training Task 7 (2 Layers)...\")\n",
    "for epoch in 1:100\n",
    "    Flux.train!(loss_2l, params(model_2l), data_loader, opt_2l)\n",
    "end\n",
    "\n",
    "test_acc_2l = evaluate_acc(model_2l, x_test, y_test)\n",
    "println(\"Task 7 Test Accuracy: $test_acc_2l\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5550430a",
   "metadata": {},
   "source": [
    "**Observation**: The addition of a non-linear layer (`ReLU`) usually correlates with a significant jump in accuracy (e.g., from ~35-40% to ~45-50%)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd49f71",
   "metadata": {},
   "source": [
    "## 7. Task 8: Activation Functions Demo\n",
    "\n",
    "Comparing ReLU, Sigmoid, and LeakyReLU on a sample matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813139b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_mat = Float32[-1 0 1 2; -2 0 1 2; 10 15 20 30; -20 -10 -5 -1]\n",
    "\n",
    "println(\"Input Matrix:\")\n",
    "display(input_mat)\n",
    "\n",
    "println(\"\\nReLU:\")\n",
    "display(relu.(input_mat))\n",
    "\n",
    "println(\"\\nSigmoid:\")\n",
    "display(sigmoid.(input_mat))\n",
    "\n",
    "println(\"\\nLeakyReLU (0.1):\")\n",
    "display(leakyrelu.(input_mat, 0.1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
