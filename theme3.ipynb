{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad50fa47",
   "metadata": {},
   "source": [
    "# Theme 3: Deep Learning Hands-On Walkthrough\n",
    "\n",
    "This notebook allows us to step through the logic of `solve_theme3.py` interactively.\n",
    "We will explore:\n",
    "1. Data Loading & Preprocessing\n",
    "2. Building a simple Linear Classifier (Single Layer Neural Network)\n",
    "3. The effects of Training Duration (Epochs)\n",
    "4. Visualizing Neural Network Weights\n",
    "5. Regularization (L2)\n",
    "6. Deepening the Network (Adding Hidden Layers)\n",
    "7. Activation Functions\n",
    "\n",
    "## Self-Learning Goal\n",
    "The goal here is not just to run code, but to understand *why* each step is necessary and what the output tells us about the model's learning process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82228ea5",
   "metadata": {},
   "source": [
    "## 1. Imports\n",
    "\n",
    "We use `numpy` for data manipulation, `tensorflow/keras` for building neural networks, `PIL` for image handling, and `matplotlib` for visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf279c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danie\\Documents\\Projects\\ml4cv\\.venv\\Lib\\site-packages\\keras\\src\\export\\tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Input, Dense, Flatten\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155bedaa",
   "metadata": {},
   "source": [
    "## 2. Helper Functions\n",
    "\n",
    "In Deep Learning, data plumbing is often 80% of the work. We need robust functions to:\n",
    "- **Load Labels**: Read class IDs from text files.\n",
    "- **Load Images**: Read image files into NumPy arrays.\n",
    "- **Normalize**: Scale pixel values to a small range (e.g., -1 to 1 or 0 to 1) to help the optimizer converge.\n",
    "- **Split**: Separate data into Testing and Validation sets to ensure we don't cheat by optimizing on our test data.\n",
    "\n",
    "*Self-Learning Note*: Notice `normalize_dataset`. It shifts data to be 0-centered (approx) by subtracting 128 and dividing by 128. This helps gradient descent steps be more uniform.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "531dd840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Functions ---\n",
    "def load_labels(filename):\n",
    "    with open(filename,'r') as file:\n",
    "       li = file.readlines()\n",
    "    label_count = len(li)\n",
    "    labels = np.empty((label_count,1), dtype='int')\n",
    "    i = 0\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            labels[i] = int(line.replace(\"\\n\", \"\"))\n",
    "            i = i + 1\n",
    "    return labels\n",
    "\n",
    "def load_images(folder, image_count, image_size):\n",
    "    array_shape = (image_count, image_size[0], image_size[1], image_size[2])\n",
    "    imageset = np.empty(array_shape, dtype='float')\n",
    "    for i in range(0,image_count):\n",
    "        fname = os.path.join(folder, 'image_' + \"{:04d}\".format(i) + '.png')\n",
    "        if not os.path.exists(fname):\n",
    "             print(f\"Warning: File {fname} not found.\")\n",
    "             continue\n",
    "        image = Image.open(fname)\n",
    "        imageset[i] = np.asarray(image)\n",
    "    return imageset\n",
    "\n",
    "def normalize_dataset(sampled_images):\n",
    "    # Scale to range [-1, 1] approx\n",
    "    sampled_images = (sampled_images.astype('float32')-128) / 128\n",
    "    return sampled_images\n",
    "\n",
    "def split_test_val(data, splitpoint):\n",
    "    # Returns (Tail, Head) - careful with indices!\n",
    "    return data[splitpoint:], data[:splitpoint]\n",
    "\n",
    "def create_model(input_shape, dense_size, classes, l2_reg=None):\n",
    "    x = Input(shape=(input_shape))\n",
    "    y = Flatten()(x)\n",
    "    \n",
    "    if l2_reg:\n",
    "        y = Dense(classes, activation='softmax', name='dense_layer', kernel_regularizer=l2_reg)(y)\n",
    "    else:\n",
    "        y = Dense(classes, activation='softmax', name='dense_layer')(y)\n",
    "        \n",
    "    model = Model(inputs=x, outputs=y)\n",
    "    return model\n",
    "\n",
    "def create_model_2_layers(input_shape, dense_size, classes):\n",
    "    x = Input(shape=(input_shape))\n",
    "    y = Flatten()(x)\n",
    "    # Task 7: 2nd dense layer usually adds 'depth' allowing complex features\n",
    "    y = Dense(dense_size, activation='relu')(y)\n",
    "    y = Dense(classes, activation='softmax', name='dense_layer_out')(y)\n",
    "    model = Model(inputs=x, outputs=y)\n",
    "    return model\n",
    "\n",
    "def visualize_weights(model, layer_name, title, filename):\n",
    "    try:\n",
    "        weights, biases = model.get_layer(layer_name).get_weights()\n",
    "    except ValueError:\n",
    "        print(f\"Layer {layer_name} not found or has no weights\")\n",
    "        return\n",
    "\n",
    "    # Weights shape: (InputDim, Units) -> (3072, 10)\n",
    "    input_shape = (32, 32, 3)\n",
    "    n_units = weights.shape[1]\n",
    "    \n",
    "    # We want to show the weights for each class (unit)\n",
    "    rows = 2\n",
    "    cols = 5 \n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(10, 5))\n",
    "    \n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i >= n_units:\n",
    "            break\n",
    "\n",
    "        weight_vector = weights[:, i]\n",
    "        # Reshape: (3072,) -> (32, 32, 3)\n",
    "        weight_image = weight_vector.reshape(input_shape)\n",
    "\n",
    "        # Normalize to 0-1 for display\n",
    "        min_val = weight_image.min()\n",
    "        max_val = weight_image.max()\n",
    "        weight_image = (weight_image - min_val) / (max_val - min_val + 1e-5)\n",
    "\n",
    "        ax.imshow(weight_image)\n",
    "        ax.set_title(f'Class {i}')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show() # Changed from savefig to show for notebook\n",
    "    print(f\"Visualization generated.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2494807",
   "metadata": {},
   "source": [
    "## 3. Loading Data\n",
    "\n",
    "We load the CIFAR-10 subset.\n",
    "We assume the `resources` folder is in the current working directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388417d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Loading Data ---\")\n",
    "base_res = 'resources' # Assumed relative to CWD\n",
    "if not os.path.exists(base_res):\n",
    "    print(f\"ERROR: '{base_res}' directory not found. Please ensure you are in the correct directory.\")\n",
    "else:\n",
    "    # 1. Load Labels\n",
    "    y_train = load_labels(os.path.join(base_res, 'training/labels.csv'))\n",
    "    \n",
    "    # 2. Load Images (based on how many labels we have)\n",
    "    x_train_raw = load_images(os.path.join(base_res, 'training'), len(y_train), (32,32,3))\n",
    "    \n",
    "    Y_test = load_labels(os.path.join(base_res, 'testing/labels.csv'))\n",
    "    X_test_raw = load_images(os.path.join(base_res, 'testing'), len(Y_test), (32,32,3))\n",
    "\n",
    "    print(\"Data Loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004bca26",
   "metadata": {},
   "source": [
    "## 4. Normalization & Splitting\n",
    "\n",
    "**Normalization**: Raw pixel values are 0-255. Neural networks prefer inputs near 0 with unit variance. We subtract 128 and divide by 128.\n",
    "\n",
    "**Splitting**: We split the 'Test' set folder into a true 'Test' set and a 'Validation' set.\n",
    "- **Validation**: Used during training to check progress.\n",
    "- **Test**: Used ONLY at the very end to report final performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be72ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = normalize_dataset(x_train_raw)\n",
    "X_test = normalize_dataset(X_test_raw)\n",
    "\n",
    "# Splitting logic from original script\n",
    "# index 0 to 2000 -> Validation\n",
    "# index 2000 to End -> Test\n",
    "splitpoint = 2000\n",
    "x_test, x_val = split_test_val(X_test, splitpoint)\n",
    "y_test, y_val = split_test_val(Y_test, splitpoint)\n",
    "\n",
    "class_count = len(np.unique(y_train))\n",
    "dims = (32, 32, 3)\n",
    "dense_sz = 100\n",
    "\n",
    "print(f\"Training set: {x_train.shape}\")\n",
    "print(f\"Validation set: {x_val.shape}\")\n",
    "print(f\"Test set: {x_test.shape}\")\n",
    "print(f\"Classes: {class_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374869d3",
   "metadata": {},
   "source": [
    "## 5. Task 3: Baseline Model (5 Epochs)\n",
    "\n",
    "We create a simple model: `Input -> Flatten -> Dense (Softmax)`.\n",
    "This is effectively Multinomial Logistic Regression.\n",
    "It has no \"hidden\" layers, so it can only learn linear boundaries between classes.\n",
    "\n",
    "We train for just 5 epochs to sanity check that learning happens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db1bea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Task 3 (Standard Model 5 epochs) ---\")\n",
    "model = create_model(dims, dense_sz, class_count)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5), metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "l1 = model.get_layer('dense_layer')\n",
    "w, b = l1.get_weights()\n",
    "print(f\"Linear Layer Weights shape: {w.shape}\")\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5, validation_data=(x_val, y_val), verbose=2)\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f'Test accuracy (5 epochs): {score[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e272c79",
   "metadata": {},
   "source": [
    "## 6. Task 4: Extended Training (100 Epochs)\n",
    "\n",
    "5 Epochs is rarely enough. Let's train for 100 epochs and plot the history.\n",
    "\n",
    "**Self-Learning Observation**:\n",
    "- If `train_acc` keeps going up but `val_acc` plateaus or goes down, we are **overfitting**.\n",
    "- If both are low, we are **underfitting**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1798d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Task 4 (Standard Model 100 epochs) ---\")\n",
    "model_t4 = create_model(dims, dense_sz, class_count)\n",
    "model_t4.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5), metrics=['accuracy'])\n",
    "history = model_t4.fit(x_train, y_train, epochs=100, validation_data=(x_val, y_val), verbose=0)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history.history['accuracy'], label='train_accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.title('Task 4: Accuracy vs Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final Train Acc: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Final Val Acc: {history.history['val_accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c4e429",
   "metadata": {},
   "source": [
    "## 7. Task 5: Visualizing Weights\n",
    "\n",
    "Since we have a linear model (no hidden layers), the weights connecting the input pixels to a class can be interpreted as a \"feature template\".\n",
    "If the model thinks a \"Car\" looks like a blob with wheels, the weights for the \"Car\" class will look like a ghost image of a car.\n",
    "\n",
    "**Observation**: High noise in these images suggests the model is memorizing statisical noise rather than learning robust features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3d2261",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Task 5 (Visualization) ---\")\n",
    "visualize_weights(model_t4, 'dense_layer', 'Task 5: Weights', 'task5_weights.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b242728",
   "metadata": {},
   "source": [
    "## 8. Task 6: L2 Regularization\n",
    "\n",
    "To reduce noise and overfitting, we apply **L2 Regularization**.\n",
    "This forces weights to be small. Large weights usually mean the model is trying too hard to fit specific noisy pixels. Small weights imply a smoother, more general solution.\n",
    "\n",
    "**Expectation**:\n",
    "- Weights should look \"smoother\" or \"blurrier\" in visualization.\n",
    "- Validation accuracy might improve (or at least the gap between train/val should close).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15711454",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Task 6 (L2 Regularization) ---\")\n",
    "model_l2 = create_model(dims, dense_sz, class_count, l2_reg=regularizers.L2(0.03))\n",
    "model_l2.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5), metrics=['accuracy'])\n",
    "history_l2 = model_l2.fit(x_train, y_train, epochs=100, validation_data=(x_val, y_val), verbose=0)\n",
    "\n",
    "# Visualize new weights\n",
    "visualize_weights(model_l2, 'dense_layer', 'Task 6: L2 Weights', 'task6_weights.png')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history_l2.history['accuracy'], label='train_accuracy')\n",
    "plt.plot(history_l2.history['val_accuracy'], label='val_accuracy')\n",
    "plt.title('Task 6: L2 Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "score_l2 = model_l2.evaluate(x_test, y_test, verbose=0)\n",
    "print(f'Test accuracy (L2, 100 epochs): {score_l2[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bd3d15",
   "metadata": {},
   "source": [
    "## 9. Task 7: Two-Layer Network\n",
    "\n",
    "Linear models can't solve XOR problems or understand hierarchical features.\n",
    "We add a helper layer (Hidden Layer) with `ReLU` activation.\n",
    "`Input -> Flatten -> Dense(100, ReLU) -> Dense(10, Softmax)`\n",
    "\n",
    "This usually boosts accuracy significantly (e.g., +10%).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98aa0c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Task 7 (2 Layers) ---\")\n",
    "model_2l = create_model_2_layers(dims, dense_sz, class_count)\n",
    "model_2l.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5), metrics=['accuracy'])\n",
    "history_2l = model_2l.fit(x_train, y_train, epochs=100, validation_data=(x_val, y_val), verbose=0)\n",
    "score_2l = model_2l.evaluate(x_test, y_test, verbose=0)\n",
    "print(f'Test accuracy (2 Layers, 100 epochs): {score_2l[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c46288",
   "metadata": {},
   "source": [
    "## 10. Task 8: Activation Functions Demo\n",
    "\n",
    "Different activations have different properties:\n",
    "- **Sigmoid**: Squisies output to [0, 1]. Can cause vanishing gradients.\n",
    "- **ReLU**: `max(0, x)`. Fast, solves vanishing gradient for positive values.\n",
    "- **LeakyReLU**: allows small negative gradient to flow.\n",
    "\n",
    "Let's test them on a dummy matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca766b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Task 8 (Activations Demo) ---\")\n",
    "input_mat = np.array([[-1, 0, 1, 2], [-2, 0, 1, 2], [10, 15, 20, 30], [-20, -10, -5, -1]]).astype(np.float32)\n",
    "\n",
    "def demo_activation(name, func):\n",
    "    out = func(input_mat)\n",
    "    print(f\"Activation: {name}\")\n",
    "    print(out.numpy())\n",
    "\n",
    "demo_activation('ReLU', keras.activations.relu)\n",
    "demo_activation('Sigmoid', keras.activations.sigmoid)\n",
    "l_relu = keras.layers.LeakyReLU(negative_slope=0.1)\n",
    "out_lrelu = l_relu(input_mat)\n",
    "print(\"Activation: LeakyReLU (0.1)\")\n",
    "print(out_lrelu.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
