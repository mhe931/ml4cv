{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Theme 5: Object Detection with YOLOv2 Tiny\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Introduction\n",
        "\n",
        "**Object Detection** is a computer vision technique that works to identify and locate objects within an image or video. Specifically, object detection draws bounding boxes around these detected objects, which allow us to locate where said objects are in (or how they move through) a given scene.\n",
        "\n",
        "**YOLO (You Only Look Once)** is a family of object detection models known for their speed and efficiency. Unlike two-stage detectors (like R-CNN) that first generate region proposals and then classify them, YOLO processes the image in a single pass, predicting bounding boxes and class probabilities directly from full images. This makes it extremely fast and suitable for real-time applications.\n",
        "\n",
        "**YOLOv2 Tiny** is a lightweight version of YOLOv2 optimized for speed and to run on devices with lower computational power. It uses the **Darknet** architecture as its backbone feature extractor.\n",
        "\n",
        "### A Note on Transposed Convolutions\n",
        "While YOLOv2 relies heavily on downsampling (pooling/strided convolutions) to aggregate features, modern architectures (like GANs or Semantic Segmentation networks like U-Net) often need to **upsample** lower-resolution feature maps back to the original image size. **Transposed Convolutions** (sometimes incorrectly called de-convolutions) are a learnable way to perform this upsampling, essentially reversing the spatial effect of a standard convolution.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 1: Theoretical Questions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "In this assignment, you will get hands-on experience in object detection. The object detection pipeline we use is YOLO version 2 Tiny built on the DarkNet feature extractor backbone. The YOLO v2 pipeline is available within TensorFlow / Keras.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "YOLOv2 Tiny is based on a neural network named \"Darknet Reference\". Navigate to https://pjreddie.com/darknet/imagenet/ and scroll down to see a comparison table \"Pre-Trained Models.\" Compared to the popular ResNet 18 network, how much less computational operations (Ops column) does Darknet Reference need?\n",
        "\n",
        "**Answer:**\n",
        "Darknet Reference requires approx **4.9x less** operations.\n",
        "*   ResNet-18 Ops: ~4.69 Billion\n",
        "*   Darknet Reference Ops: ~0.96 Billion\n",
        "*   Ratio: $4.69 / 0.96 \u0007pprox 4.885$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Click on \"cfg\" on the table's row titled \"Darknet Reference\". What is the layer structure of Darknet Reference? (how many layers and which type?)\n",
        "\n",
        "**Answer:**\n",
        "The Darknet Reference model has **16 layers** (counting convs and fully connected/softmax, excluding pooling which is an operation).\n",
        "The structure typically follows a pattern of Convolution followed by Maxpooling.\n",
        "Structure: `conv-maxp-conv-maxp-conv-maxp-conv-maxp-conv-maxp-conv-avgp-conv-smax`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before proceeding to Question 3, we set up our environment by cloning the repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!git clone https://github.com/jboutell/keras-YOLOv3-model-set.git\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "The Darknet Reference network is used as the feature extractor part of YOLO v2 Tiny. Compare the YOLO v2 Tiny network structure to the Darknet Reference structure. What differences can you find?\n",
        "(The YOLO v2 Tiny network can be found in `keras-YOLOv3-model-set/cfg/yolov2-tiny.cfg`)\n",
        "\n",
        "**Answer:**\n",
        "The **last 3 layers** differ significantly.\n",
        "*   **Darknet Reference** ends with: `Global Average Pooling` -> `Convolution` (1000 for ImageNet) -> `Softmax` (Classification).\n",
        "*   **YOLOv2 Tiny** ends with: `Convolution` -> `Convolution` -> `Region` (Detection).\n",
        "Basically, the classification head is replaced by a detection head.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 4\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "What is the purpose of the last layer of Darknet Reference? What is the purpose of the last layer of YOLO v2 Tiny?\n",
        "\n",
        "**Answer:**\n",
        "*   **Darknet Reference**: It is a classification architecture. Its last `SoftMax` layer classifies the extracted features into per-class probabilities (e.g., \"This image contains a Cat\").\n",
        "*   **YOLO v2 Tiny**: It is an object detection architecture. Its last layer output determines **bounding boxes** [x, y, w, h], **objectness scores**, and **class probabilities** for each box grid. It outputs a volume of predictions rather than a single vector.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 5\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Observe the model summary (layers) that are printed during model execution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# We interpret the answers based on a standard YOLOv2 Tiny implementation (416x416 input).\n",
        "# If we were to run the code in an environment with the library installed:\n",
        "# import os\n",
        "# os.chdir('keras-YOLOv3-model-set')\n",
        "# # Code to load model and print summary would typically involve:\n",
        "# # model = load_model(...)\n",
        "# # model.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**a) What is the three-dimensional output size of the last conv layer of YOLO v2 Tiny?**\n",
        "**Answer:** `13 x 13 x 425`\n",
        "\n",
        "**b) Looking at the three-dimensional output size, what is the size of the feature grid?**\n",
        "**Answer:** `13 x 13`. This means the image is divided into a 13x13 grid, and each cell is responsible for detecting objects centered within it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 6\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Read the YOLO v2 paper 'YOLO9000: better, faster, stronger' and answer the following questions:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**a) How many bounding boxes at each (feature grid) cell does YOLO v2 use for object detection?**\n",
        "**Answer:** The paper states there are **5 anchor boxes** (priors) per feature grid cell.\n",
        "\n",
        "**b) The Yolov2-Tiny model in our github repository has been trained to recognize 80 object classes. Now that you know how many bounding boxes there are per (feature grid) cell, can you explain the size of the last dimension (425) of the model output?**\n",
        "**Answer:**\n",
        "Each grid cell outputs a vector for each of the 5 anchor boxes.\n",
        "For *one* bounding box, we predict:\n",
        "*   4 coordinates: $t_x, t_y, t_w, t_h$\n",
        "*   1 objectness score: $P(Object)$\n",
        "*   80 class probabilities: $P(Class_i | Object)$\n",
        "Total per box = $4 + 1 + 80 = 85$.\n",
        "With 5 boxes, the depth is $5 \\times 85 = 425$.\n",
        "\n",
        "**c) Open the file configs/yolov2-tiny_anchors.txt â€“ what is the meaning of the ten decimal values shown in this file?**\n",
        "**Answer:**\n",
        "The ten values represent the **width and height** of the 5 anchor boxes (priors) relative to the grid size.\n",
        "Pairs: $(w_1, h_1), (w_2, h_2), ..., (w_5, h_5)$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 7\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "The Yolo v2 postprocessing is implemented in the file `yolo2/postprocessing_np.py`. Follow through the function and briefly explain what happens in the sub-parts of post-processing.\n",
        "\n",
        "**Answer:**\n",
        "*   `Yolo_decode`: Unpacks the raw tensor $(13, 13, 425)$ into separate components: box coordinates, objectness scores, and class scores. It applies the sigmoid activation to $x, y, objectness$ and exponentiates $w, h$ with anchors.\n",
        "*   `Yolo_correct_boxes`: Converts the relative grid coordinates of the bounding boxes to the actual input image coordinates (pixels).\n",
        "*   `Yolo_handle_predictions`: Filters out boxes with low objectness scores. It usually applies **Non-Maximum Suppression (NMS)** here to remove overlapping duplicate boxes for the same object, keeping only the best one.\n",
        "*   `Yolo_adjust_boxes`: Adjusts box aspect ratios or final corner coordinates $(ymin, xmin, ymax, xmax)$ for drawing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 3: Visual Results and Discussion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "To verify our model, we run it on a sample image (e.g., standard 'dog.jpg').\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Example code flow to run prediction (pseudocode/repo-specific)\n",
        "# !python tools/image_demo.py --image images/dog.jpg --model configs/yolov2-tiny.h5\n",
        "#\n",
        "# import matplotlib.pyplot as plt\n",
        "# img = plt.imread('output/dog_prediction.jpg')\n",
        "# plt.imshow(img)\n",
        "# plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**Observations:**\n",
        "The model should correctly identify the dog, the bicycle, and potentially the truck in the background. The bounding boxes will display the class label and the confidence score.\n",
        "\n",
        "**Why YOLOv2 Tiny?**\n",
        "*   **Speed**: Due to fewer layers (Darknet vs ResNet) and the \"one-shot\" nature (no region proposals), it is exceptionally fast.\n",
        "*   **Efficiency**: The \"Tiny\" variant reduces parameters further, making it viable for mobile or embedded devices.\n",
        "*   **Trade-off**: It might be less accurate than full YOLOv2 or Faster R-CNN on small or crowded objects, but the speed gain is massive.\n",
        "\n",
        "**Connection to Transposed Convolutions:**\n",
        "While YOLOv2 uses `maxpool` to downsample to 13x13, semantic segmentation models (which label *every pixel*) need to go back up to the original size. They use **Transposed Convolutions** to learn how to \"paint\" the low-res features back onto the high-res canvas. This is crucial for understanding how we move from Detection (boxes on a grid) to Segmentation (pixel masks).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "We have explored the architecture of YOLOv2 Tiny, understood its output tensor shape $(13 \\times 13 \\times 425)$, and analyzed how raw predictions are post-processed into usable bounding boxes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install imgaug\n",
        "# !pip install \"numpy<2.0\"\n",
        "!pip install tensorflow-model-optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-27 09:03:49.028299: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-27 09:03:51.325481: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "c:\\Users\\danie\\Documents\\Projects\\ml4cv\\.venv\\Lib\\site-packages\\keras\\src\\export\\tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
            "  if not hasattr(np, \"object\"):\n",
            "Traceback (most recent call last):\n",
            "  File \u001b[35m\"c:\\Users\\danie\\Documents\\Projects\\ml4cv\\keras-YOLOv3-model-set\\yolo.py\"\u001b[0m, line \u001b[35m17\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
            "    from tensorflow_model_optimization.sparsity import keras as sparsity\n",
            "\u001b[1;35mModuleNotFoundError\u001b[0m: \u001b[35mNo module named 'tensorflow_model_optimization'\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# !git clone https://github.com/david8862/keras-YOLOv3-model-set.git/\n",
        "\n",
        "\n",
        "!python ./keras-YOLOv3-model-set/yolo.py --model_type=tiny_yolo --darknet --weights_path=weights/yolov2-tiny.h5 --anchors_path=configs/yolov-tiny_anchors.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from PIL import Image\n",
        "display(Image.open('output.png', mode='r'))\n",
        "\n",
        "# First example: Nearest-neighbor upsampling mimic using Conv2DTranspose\n",
        "from numpy import asarray\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2DTranspose\n",
        "\n",
        "# define input data\n",
        "X = asarray([[1, 2],\n",
        "             [3, 4]])\n",
        "\n",
        "# show input data for context\n",
        "print(X)\n",
        "\n",
        "# reshape input data into one sample with a channel\n",
        "X = X.reshape((1, 2, 2, 1))\n",
        "\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Conv2DTranspose(1, (2,2), strides=(2,2), padding='same', input_shape=(2, 2, 1)))\n",
        "model.summary()\n",
        "\n",
        "# define weights that mimic nearest neighbor upsampling\n",
        "weights = [asarray([[[[1, 1],\n",
        "                     [1, 1]]]]), asarray([0])]\n",
        "\n",
        "weights[0] = weights[0].reshape(2,2,1,1)\n",
        "model.set_weights(weights)\n",
        "\n",
        "yhat = model.predict(X)\n",
        "\n",
        "# reshape output to remove channel to make printing easier\n",
        "yhat = yhat.reshape((4, 4))\n",
        "print(yhat)\n",
        "\n",
        "# Second example: Bilinear interpolation mimic using Conv2DTranspose\n",
        "from numpy import asarray\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2DTranspose\n",
        "\n",
        "# define input data\n",
        "X = asarray([[1, 2],\n",
        "             [3, 4]])\n",
        "\n",
        "# show input data for context\n",
        "print(X)\n",
        "\n",
        "# reshape input data into one sample with a channel\n",
        "X = X.reshape((1, 2, 2, 1))\n",
        "\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Conv2DTranspose(1, (4,4), strides=(2,2), padding='same', input_shape=(2, 2, 1)))\n",
        "model.summary()\n",
        "\n",
        "# define weights that mimic bilinear interpolation\n",
        "weights = [asarray([[[[0.0625, 0.1875, 0.1875, 0.0625],\n",
        "                     [0.1875, 0.5625, 0.5625, 0.1875],\n",
        "                     [0.1875, 0.5625, 0.5625, 0.1875],\n",
        "                     [0.0625, 0.1875, 0.1875, 0.0625]]]]), asarray([0])]\n",
        "\n",
        "weights[0] = weights[0].reshape(4,4,1,1)\n",
        "model.set_weights(weights)\n",
        "\n",
        "yhat = model.predict(X)\n",
        "\n",
        "# reshape output to remove channel to make printing easier\n",
        "yhat = yhat.reshape((4, 4))\n",
        "print(yhat)\n",
        "\n",
        "# Third example: Upscaling a low-resolution image (e.g., a dog) using bilinear-like transpose convolution\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "from numpy import asarray\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2DTranspose\n",
        "\n",
        "# Define the model (same as second example)\n",
        "model = Sequential()\n",
        "model.add(Conv2DTranspose(1, (4,4), strides=(2,2), padding='same', input_shape=(32, 32, 1)))\n",
        "\n",
        "# Bilinear-like weights\n",
        "weights = [asarray([[[[0.0625, 0.1875, 0.1875, 0.0625],\n",
        "                     [0.1875, 0.5625, 0.5625, 0.1875],\n",
        "                     [0.1875, 0.5625, 0.5625, 0.1875],\n",
        "                     [0.0625, 0.1875, 0.1875, 0.0625]]]]), asarray([0])]\n",
        "\n",
        "weights[0] = weights[0].reshape(4,4,1,1)\n",
        "model.set_weights(weights)\n",
        "\n",
        "# Load and display original image\n",
        "img_bgr = cv2.imread('example.png', cv2.IMREAD_COLOR)  # Replace with your low-res image path\n",
        "img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(img_rgb)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Split channels and reshape\n",
        "img_r = img_bgr[:,:,2].reshape((1, 32, 32, 1))  # Note: BGR order, R is [:,:,2]\n",
        "img_g = img_bgr[:,:,1].reshape((1, 32, 32, 1))\n",
        "img_b = img_bgr[:,:,0].reshape((1, 32, 32, 1))\n",
        "\n",
        "# Upscale each channel\n",
        "up_r = model.predict(img_r).reshape((64, 64))\n",
        "up_g = model.predict(img_g).reshape((64, 64))\n",
        "up_b = model.predict(img_b).reshape((64, 64))\n",
        "\n",
        "# Merge and display upscaled image\n",
        "img_up = cv2.merge((up_b, up_g, up_r)).astype('uint8')  # Back to BGR for correct colors if needed\n",
        "plt.imshow(cv2.cvtColor(img_up, cv2.COLOR_BGR2RGB))\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
